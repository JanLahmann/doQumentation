{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: install Qiskit (runs automatically in Colab, no-op in Binder)\n",
    "!pip install -q qiskit qiskit-aer qiskit-ibm-runtime pylatexenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional dependencies for this notebook\n",
    "!pip install -q imbalanced-learn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d1e3ec",
   "metadata": {},
   "source": "# ハイブリッド量子強化アンサンブル分類（電力グリッド安定性ワークフロー）\n\n*使用量の目安：Eagle r3プロセッサ上の各ジョブにつきQPU時間で約20分。（注意：これはあくまで目安です。実際の実行時間は異なる場合があります。）*\n## 背景\nこのチュートリアルでは、古典アンサンブルを量子最適化ステップで強化するハイブリッド量子・古典ワークフローを実演します。Multiverse Computingの「Singularity Machine Learning – Classification」（Qiskit Function）を使用して、従来型の学習器プール（例：決定木、k-NN、ロジスティック回帰）を訓練し、その後量子レイヤーでプールを精錬することで多様性と汎化性能を向上させます。目的は実用的なものです。実際のグリッド安定性予測タスクにおいて、同じデータ分割の下で強力な古典ベースラインと量子最適化された代替手法を比較し、量子ステップがどこで有効か、またそのコストがどの程度かを確認できるようにします。\n\nこれが重要である理由は以下の通りです。多数の弱学習器から優れたサブセットを選択することは、アンサンブルサイズに応じて急速に増大する組合せ最適化問題です。ブースティング、バギング、スタッキングなどの古典的ヒューリスティクスは中規模では良好に機能しますが、大規模で冗長なモデルライブラリを効率的に探索するのは困難になることがあります。この関数は量子アルゴリズム、具体的にはQAOA（および他の構成ではオプションでVQE）を統合し、古典学習器の訓練後にその探索空間をより効果的に探索することで、汎化性能に優れたコンパクトで多様なサブセットを見つける可能性を高めます。\n\n重要な点として、データスケールは量子ビット数による制限を受けません。データに関する重い処理（前処理、学習器プールの訓練、評価）は古典的なまま行われ、数百万のサンプルを扱うことができます。量子ビットは量子選択ステップで使用されるアンサンブルサイズのみを決定します。この分離こそが、現在のハードウェアでこのアプローチを実用的にしている理由です。データとモデル訓練には馴染みのあるscikit-learnワークフローを維持しながら、Qiskit Functionsのクリーンなアクションインターフェースを通じて量子ステップを呼び出すことができます。\n\n実際には、アンサンブルに様々な種類の学習器を提供できます（例：決定木、ロジスティック回帰、k-NNなど）が、決定木が最も良い性能を発揮する傾向があります。オプティマイザは一貫してより強力なアンサンブルメンバーを優先し、異種の学習器が提供された場合、線形回帰器のような弱いモデルは通常、決定木のようなより表現力の高いモデルに置き換えられて除去されます。\n\nこのチュートリアルで行うこと：グリッド安定性データセットの準備とバランス調整、古典的なAdaBoostベースラインの確立、アンサンブル幅と正則化を変化させた複数の量子構成の実行、Qiskit Serverless経由でのIBM&reg;シミュレータまたはQPU上での実行、そしてすべての実行における精度、適合率、再現率、F1スコアの比較を行います。この過程で、関数のアクションパターン（`create`、`fit`、`predict`、`fit_predict`、`create_fit_predict`）と主要な制御パラメータを使用します：\n- 正則化タイプ：直接的なスパース化のための`onsite`（λ）と、相互作用項とオンサイト項の比率ベースのトレードオフのための`alpha`\n- 自動正則化：目標選択比率を指定して`regularization=\"auto\"`を設定し、スパース性を自動的に適応させます\n- オプティマイザオプション：シミュレータ対QPU、反復回数、古典オプティマイザとそのオプション、トランスパイレーション深度、ランタイムサンプラー/エスティメータ設定\n\nドキュメントのベンチマークによると、困難な問題において学習器数（量子ビット数）が増加するにつれて精度が向上し、量子分類器は同等の古典アンサンブルと同等以上の性能を達成しています。このチュートリアルでは、ワークフロー全体をエンドツーエンドで再現し、アンサンブル幅の増加や適応的正則化への切り替えが、合理的なリソース使用量でより良いF1スコアをもたらすタイミングを検証します。その結果として、量子最適化ステップが実際のアプリケーションにおいて古典的アンサンブル学習を置き換えるのではなく、いかに補完できるかについての実践的な理解が得られます。\n## 要件\nこのチュートリアルを開始する前に、Python環境に以下のパッケージがインストールされていることを確認してください：\n\n- `qiskit[visualization]~=2.1.0`\n- `qiskit-serverless~=0.24.0`\n- `qiskit-ibm-runtime v0.40.1`\n- `qiskit-ibm-catalog~=0.8.0`\n- `scikit-learn==1.5.2`\n- `pandas>=2.0.0,<3.0.0`\n- `imbalanced-learn~=0.12.3`\n## セットアップ\nこのセクションでは、Qiskit Serverlessクライアントを初期化し、Multiverse Computingが提供するSingularity Machine Learning – Classification関数をロードします。\nQiskit Serverlessを使用すると、リソース管理を気にすることなく、IBMのマネージドクラウドインフラストラクチャ上でハイブリッド量子・古典ワークフローを実行できます。\n認証してQiskit Functionsにアクセスするには、IBM Quantum PlatformのAPIキーとクラウドリソース名（CRN）が必要です。\n### データセットのダウンロード\nこのチュートリアルを実行するために、ラベル付きの電力系統センサーデータを含む前処理済みの**グリッド安定性分類データセット**を使用します。\n以下のセルは、必要なフォルダ構造を自動的に作成し、`wget`を使用してトレーニングファイルとテストファイルの両方を環境に直接ダウンロードします。\nこれらのファイルが既にローカルにある場合でも、バージョンの一貫性を確保するために安全に上書きされます。"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6f69b77",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "8bf80006",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "55b94021",
   "metadata": {},
   "source": ""
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7db2e559",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "d4fe7ee1-21ce-445c-b151-598cd4cf9227",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a32efb3-a425-4c02-804b-65029ecffb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_tutorial/grid_ 100%[===================>] 612.94K  --.-KB/s    in 0.01s   \n",
      "data_tutorial/grid_ 100%[===================>] 108.19K  --.-KB/s    in 0.006s  \n",
      "Dataset files downloaded:\n",
      "-rw-r--r-- 1 coder coder 109K Nov  8 18:50 data_tutorial/grid_stability/test.csv\n",
      "-rw-r--r-- 1 coder coder 613K Nov  8 18:50 data_tutorial/grid_stability/train.csv\n"
     ]
    }
   ],
   "source": [
    "## Download dataset for Grid Stability Classification\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "!mkdir -p data_tutorial/grid_stability\n",
    "\n",
    "# Download the training and test sets from the official Qiskit documentation repo\n",
    "!wget -q --show-progress -O data_tutorial/grid_stability/train.csv \\\n",
    "  https://raw.githubusercontent.com/Qiskit/documentation/main/datasets/tutorials/grid_stability/train.csv\n",
    "\n",
    "!wget -q --show-progress -O data_tutorial/grid_stability/test.csv \\\n",
    "  https://raw.githubusercontent.com/Qiskit/documentation/main/datasets/tutorials/grid_stability/test.csv\n",
    "\n",
    "# Check the files have been downloaded\n",
    "!echo \"Dataset files downloaded:\"\n",
    "!ls -lh data_tutorial/grid_stability/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9aa84f-ab37-412c-b056-7043b73380fa",
   "metadata": {},
   "source": [
    "### Import required packages\n",
    "\n",
    "In this section, we import all Python packages and Qiskit modules used throughout the tutorial.\n",
    "These include core scientific libraries for data handling and model evaluation - such as `NumPy`, `pandas`, and `scikit-learn` - along with visualization tools and Qiskit components for running the quantum-enhanced model.\n",
    "We also import the `QiskitRuntimeService` and `QiskitFunctionsCatalog` to connect with IBM Quantum&reg; services and access the Singularity Machine Learning function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c654f5-8355-4f67-b79d-c2b1c29ccc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from qiskit_ibm_catalog import QiskitFunctionsCatalog\n",
    "from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b389b34-02e8-4a63-ae24-0bf348647b48",
   "metadata": {},
   "source": "### 必要なパッケージのインポート\nこのセクションでは、チュートリアル全体で使用するすべてのPythonパッケージとQiskitモジュールをインポートします。\nこれらには、データ処理とモデル評価のための主要な科学計算ライブラリ（`NumPy`、`pandas`、`scikit-learn`など）、可視化ツール、および量子強化モデルを実行するためのQiskitコンポーネントが含まれます。\nまた、IBM Quantum&reg;サービスに接続しSingularity Machine Learning関数にアクセスするために、`QiskitRuntimeService`と`QiskitFunctionsCatalog`もインポートします。"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a68bf7df-914c-4b2a-827f-657578503750",
   "metadata": {},
   "outputs": [],
   "source": [
    "IBM_TOKEN = \"\"\n",
    "IBM_INSTANCE_TEST = \"\"\n",
    "IBM_INSTANCE_QUANTUM = \"\"\n",
    "FUNCTION_NAME = \"multiverse/singularity\"\n",
    "RANDOM_STATE: int = 123\n",
    "TRAIN_PATH = \"data_tutorial/grid_stability/train.csv\"\n",
    "TEST_PATH = \"data_tutorial/grid_stability/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4709dc1c-b380-49f1-95c7-89197aa5e147",
   "metadata": {},
   "source": "### 定数変数の設定"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc380c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to IBM Qiskit Serverless and loaded the Singularity function.\n",
      "Catalog: <QiskitFunctionsCatalog>\n",
      "Singularity function: QiskitFunction(multiverse/singularity)\n"
     ]
    }
   ],
   "source": [
    "service = QiskitRuntimeService(\n",
    "    token=IBM_TOKEN,\n",
    "    channel=\"ibm_quantum_platform\",\n",
    "    instance=IBM_INSTANCE_QUANTUM,\n",
    ")\n",
    "\n",
    "backend = service.least_busy()\n",
    "catalog = QiskitFunctionsCatalog(\n",
    "    token=IBM_TOKEN,\n",
    "    instance=IBM_INSTANCE_TEST,\n",
    "    channel=\"ibm_quantum_platform\",\n",
    ")\n",
    "singularity = catalog.load(FUNCTION_NAME)\n",
    "print(\n",
    "    \"Successfully connected to IBM Qiskit Serverless and loaded the Singularity function.\"\n",
    ")\n",
    "print(\"Catalog:\", catalog)\n",
    "print(\"Singularity function:\", singularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d6a559-118a-4aa9-874d-9c009b5da60c",
   "metadata": {},
   "source": "### IBM Quantumへの接続とSingularity関数のロード\n次に、IBM Quantumサービスで認証を行い、Qiskit Functions CatalogからSingularity Machine Learning – Classification関数をロードします。\n`QiskitRuntimeService`は、APIトークンとインスタンスCRNを使用してIBM Quantum Platformへの安全な接続を確立し、量子バックエンドへのアクセスを可能にします。\nその後、`QiskitFunctionsCatalog`を使用して名前（`\"multiverse/singularity\"`）でSingularity関数を取得し、後でハイブリッド量子・古典計算に呼び出せるようにします。\nセットアップが成功すると、関数が正しくロードされたことを示す確認メッセージが表示されます。"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46bc841e-7365-4508-b6bf-ae57db6050e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load data from the given path to X and y arrays.\"\"\"\n",
    "    df: pd.DataFrame = pd.read_csv(data_path)\n",
    "    return df.iloc[:, :-1].values, df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "def evaluate_predictions(predictions, y_true):\n",
    "    \"\"\"Compute and print accuracy, precision, recall, and F1 score.\"\"\"\n",
    "    accuracy = accuracy_score(y_true, predictions)\n",
    "    precision = precision_score(y_true, predictions)\n",
    "    recall = recall_score(y_true, predictions)\n",
    "    f1 = f1_score(y_true, predictions)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1:\", f1)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "988ee237",
   "metadata": {},
   "source": [
    "## Step 1: Map classical inputs to a quantum problem\n",
    "\n",
    "We begin by preparing the dataset for hybrid quantum–classical experimentation. The goal of this step is to convert the raw grid-stability data into balanced training, validation, and test splits that can be used consistently by both classical and quantum workflows. Maintaining identical splits ensures that later performance comparisons are fair and reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c084cde-a5cf-4661-a00c-aa243c8b0e44",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing\n",
    "\n",
    "We first load the training and test CSV files, create a validation split, and balance the dataset using random over-sampling. Balancing prevents bias toward the majority class and provides a more stable learning signal for both classical and quantum ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db0e914-a8f2-4a04-bec7-c15bac8104b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "  X_train_bal: (5104, 12)\n",
      "  y_train_bal: (5104,)\n",
      "  X_val: (850, 12)\n",
      "  y_val: (850,)\n",
      "  X_test: (750, 12)\n",
      "  y_test: (750,)\n"
     ]
    }
   ],
   "source": [
    "# Load and upload the data\n",
    "X_train, y_train = load_data(TRAIN_PATH)\n",
    "X_test, y_test = load_data(TEST_PATH)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Balance the dataset through over-sampling of the positive class\n",
    "ros = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "X_train_bal, y_train_bal = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_train_bal:\", X_train_bal.shape)\n",
    "print(\"  y_train_bal:\", y_train_bal.shape)\n",
    "print(\"  X_val:\", X_val.shape)\n",
    "print(\"  y_val:\", y_val.shape)\n",
    "print(\"  X_test:\", X_test.shape)\n",
    "print(\"  y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c214fd-09a8-4fa5-ab83-b0c71ad615d4",
   "metadata": {},
   "source": "### ヘルパー関数の定義\nメインの実験を実行する前に、データの読み込みとモデル評価を効率化するいくつかの小さなユーティリティ関数を定義します。\n- `load_data()`は、入力CSVファイルをNumPy配列に読み込み、`scikit-learn`および量子ワークフローとの互換性のために特徴量とラベルを分離します。\n- `evaluate_predictions()`は、主要なパフォーマンス指標（精度、適合率、再現率、F1スコア）を計算し、タイミング情報が提供された場合はオプションで実行時間を報告します。\n\nこれらのヘルパー関数は、ノートブックの後半で繰り返される操作を簡略化し、古典分類器と量子分類器の両方で一貫した指標の報告を保証します。"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38ecd608-35b4-4e58-8a16-85ef98bf024a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical AdaBoost baseline:\n",
      "Accuracy: 0.7893333333333333\n",
      "Precision: 1.0\n",
      "Recall: 0.7893333333333333\n",
      "F1: 0.8822652757078987\n"
     ]
    }
   ],
   "source": [
    "# ----- Classical baseline: AdaBoost -----\n",
    "baseline = AdaBoostClassifier(n_estimators=60, random_state=RANDOM_STATE)\n",
    "baseline.fit(X_train_bal, y_train_bal)\n",
    "baseline_pred = baseline.predict(X_test)\n",
    "print(\"Classical AdaBoost baseline:\")\n",
    "_ = evaluate_predictions(baseline_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f36e3",
   "metadata": {},
   "source": "## ステップ1：古典的な入力を量子問題にマッピングする\nハイブリッド量子・古典実験のためにデータセットを準備するところから始めます。このステップの目的は、生のグリッド安定性データを、古典ワークフローと量子ワークフローの両方で一貫して使用できるバランスの取れた訓練セット、検証セット、テストセットに変換することです。同一の分割を維持することで、後のパフォーマンス比較が公平かつ再現可能になります。\n### データの読み込みと前処理\nまず、トレーニングとテストのCSVファイルを読み込み、検証用分割を作成し、ランダムオーバーサンプリングを使用してデータセットのバランスを調整します。バランス調整により多数派クラスへの偏りを防ぎ、古典および量子アンサンブルモデルの両方により安定した学習信号を提供します。"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c0d3f9-691b-449d-83ca-e6bfce2d6b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured hardware optimization profile:\n",
      "  simulator: False\n",
      "  num_solutions: 100000\n",
      "  reps: 3\n",
      "  optimization_level: 3\n",
      "  num_transpiler_runs: 30\n",
      "  classical_optimizer: COBYLA\n",
      "  classical_optimizer_options: {'maxiter': 60}\n",
      "  estimator_options: None\n",
      "  sampler_options: None\n"
     ]
    }
   ],
   "source": [
    "# QAOA / runtime configuration for best results on hardware\n",
    "optimizer_options = {\n",
    "    \"simulator\": False,  # set True to test locally without QPU\n",
    "    \"num_solutions\": 100_000,  # broaden search over candidate ensembles\n",
    "    \"reps\": 3,  # QAOA depth (circuit layers)\n",
    "    \"optimization_level\": 3,  # transpilation effort\n",
    "    \"num_transpiler_runs\": 30,  # explore multiple layouts\n",
    "    \"classical_optimizer\": \"COBYLA\",  # robust default for this landscape\n",
    "    \"classical_optimizer_options\": {\n",
    "        \"maxiter\": 60  # practical convergence budget\n",
    "    },\n",
    "    # You can pass backend-specific options; leaving None uses least-busy routing\n",
    "    \"estimator_options\": None,\n",
    "    \"sampler_options\": None,\n",
    "}\n",
    "\n",
    "print(\"Configured hardware optimization profile:\")\n",
    "for key, value in optimizer_options.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4d480b3",
   "metadata": {},
   "source": [
    "## Step 3: Execute using Qiskit primitives\n",
    "\n",
    "We now execute the full workflow using the Singularity function’s `create_fit_predict` action to train, optimize, and evaluate the `QuantumEnhancedEnsembleClassifier` end-to-end on IBM infrastructure. The function builds the ensemble, applies quantum optimization through Qiskit primitives, and returns both predictions and job metadata (including runtime and resource usage). The classical data split from Step 1 is reused for reproducibility, with validation data passed through `fit_params` so the optimization can tune hyperparameters internally while keeping the held-out test set untouched.\n",
    "\n",
    "In this step, we explore several configurations of the quantum ensemble to understand how key parameters - specifically `num_learners` and `regularization` - affect both result quality and QPU usage.\n",
    "- `num_learners` determines the ensemble width (and implicitly, the number of qubits), influencing the model’s capacity and computational cost.\n",
    "- `regularization` controls sparsity and overfitting, shaping how many learners remain active after optimization.\n",
    "\n",
    "By varying these parameters, we can see how ensemble width and regularization interact: increasing width typically improves F1 but costs more QPU time, while stronger or adaptive regularization can improve generalization at roughly the same hardware footprint. The next subsections walk through three representative configurations to illustrate these effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da41d4d-f06d-4b67-a8ee-b07fc0289558",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "This configuration uses `num_learners = 10` and `regularization = 7`.\n",
    "\n",
    "- `num_learners` controls the ensemble width — effectively the number of weak learners combined and, on quantum hardware, the **number of qubits required**. A larger value expands the combinatorial search space and can improve accuracy and recall, but also increases circuit width, compilation time, and overall QPU usage.\n",
    "- `regularization` sets the penalty strength for including additional learners. With the default \"onsite\" regularization, higher values enforce stronger sparsity (fewer learners kept), while lower values allow more complex ensembles.\n",
    "\n",
    "This setup provides a low-cost baseline, showing how a small ensemble behaves before scaling width or tuning sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e702c986-fd7f-4ea1-a93c-8cd5ef0c4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem scale and regularization\n",
    "NUM_LEARNERS = 10\n",
    "REGULARIZATION = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06c261c1-a3c1-42c9-a522-63f3fc01a970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Submitting quantum-enhanced ensemble job --\n",
      "Action status: ok\n",
      "Action message: Classifier created, fitted, and predicted.\n",
      "Metadata: {'resource_usage': {'RUNNING: MAPPING': {'CPU_TIME': 267.05158376693726}, 'RUNNING: WAITING_QPU': {'CPU_TIME': 3336.8785166740417}, 'RUNNING: POST_PROCESSING': {'CPU_TIME': 152.4274561405182}, 'RUNNING: EXECUTING_QPU': {'QPU_TIME': 1550.1889700889587}}}\n",
      "Accuracy: 0.868\n",
      "Precision: 1.0\n",
      "Recall: 0.868\n",
      "F1: 0.9293361884368309\n"
     ]
    }
   ],
   "source": [
    "# ----- Quantum-enhanced ensemble on IBM hardware -----\n",
    "print(\"\\n-- Submitting quantum-enhanced ensemble job --\")\n",
    "job_1 = singularity.run(\n",
    "    action=\"create_fit_predict\",\n",
    "    name=\"grid_stability_qeec\",\n",
    "    quantum_classifier=\"QuantumEnhancedEnsembleClassifier\",\n",
    "    num_learners=NUM_LEARNERS,\n",
    "    regularization=REGULARIZATION,\n",
    "    optimizer_options=optimizer_options,  # from Step 2\n",
    "    backend_name=backend,  # least-busy compatible backend\n",
    "    instance=IBM_INSTANCE_QUANTUM,\n",
    "    random_state=RANDOM_STATE,\n",
    "    X_train=X_train_bal,\n",
    "    y_train=y_train_bal,\n",
    "    X_test=X_test,\n",
    "    fit_params={\"validation_data\": (X_val, y_val)},\n",
    "    options={\"save\": False},\n",
    ")\n",
    "result_1 = job_1.result()\n",
    "print(\"Action status:\", result_1.get(\"status\"))\n",
    "print(\"Action message:\", result_1.get(\"message\"))\n",
    "print(\"Metadata:\", result_1.get(\"metadata\"))\n",
    "qeec_pred_job_1 = np.array(result_1[\"data\"][\"predictions\"])\n",
    "_ = evaluate_predictions(qeec_pred_job_1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf0a3ae-b5b8-43d7-b72d-5a9cf6c61060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantum job status: DONE\n"
     ]
    }
   ],
   "source": [
    "status_1 = job_1.status()\n",
    "print(\"\\nQuantum job status:\", status_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62b414-db77-45aa-bf2c-a8537fd7eba0",
   "metadata": {},
   "source": "## ステップ2：量子ハードウェア実行のための問題の最適化\nアンサンブル選択タスクは組合せ最適化問題として定式化されます。ここでは、各弱学習器が二値決定変数であり、目的関数は正則化項を通じて精度とスパース性のバランスを取ります。`QuantumEnhancedEnsembleClassifier`はIBMハードウェア上でQAOAを使用してこの問題を解きますが、シミュレータベースの探索も可能です。`optimizer_options`はハイブリッドループを制御します：`simulator=False`は回路を選択したQPUにルーティングし、`num_solutions`は探索幅を増加させ、`classical_optimizer_options`（内部の古典オプティマイザ用）は収束を制御します。約60回の反復が品質と実行時間のバランスとして適切です。ランタイムオプション（適度な回路深度（`reps`）や標準的なトランスパイレーション努力など）は、デバイス間で堅牢な性能を確保するのに役立ちます。以下の構成はハードウェア実行に使用する「最良結果」プロファイルです。`simulator=True`に切り替えることで、QPU時間を消費せずにワークフローをドライランするための純粋なシミュレーションバリアントを作成することもできます。"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5fe64b0-2713-4b65-b768-10fa5d8dbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem scale and regularization\n",
    "NUM_LEARNERS = 30\n",
    "REGULARIZATION = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18a22fed-41c2-4407-88d4-02fbd32f3320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Submitting quantum-enhanced ensemble job --\n",
      "Action status: ok\n",
      "Action message: Classifier created, fitted, and predicted.\n",
      "QPU Time: {'resource_usage': {'RUNNING: MAPPING': {'CPU_TIME': 680.2116754055023}, 'RUNNING: WAITING_QPU': {'CPU_TIME': 80.80395102500916}, 'RUNNING: POST_PROCESSING': {'CPU_TIME': 154.4466371536255}, 'RUNNING: EXECUTING_QPU': {'QPU_TIME': 1095.822762966156}}}\n",
      "Accuracy: 0.8946666666666667\n",
      "Precision: 1.0\n",
      "Recall: 0.8946666666666667\n",
      "F1: 0.944405348346235\n"
     ]
    }
   ],
   "source": [
    "# ----- Quantum-enhanced ensemble on IBM hardware -----\n",
    "print(\"\\n-- Submitting quantum-enhanced ensemble job --\")\n",
    "job_2 = singularity.run(\n",
    "    action=\"create_fit_predict\",\n",
    "    name=\"grid_stability_qeec\",\n",
    "    quantum_classifier=\"QuantumEnhancedEnsembleClassifier\",\n",
    "    num_learners=NUM_LEARNERS,\n",
    "    regularization=REGULARIZATION,\n",
    "    optimizer_options=optimizer_options,  # from Step 2\n",
    "    backend_name=backend,  # least-busy compatible backend\n",
    "    instance=IBM_INSTANCE_QUANTUM,\n",
    "    random_state=RANDOM_STATE,\n",
    "    X_train=X_train_bal,\n",
    "    y_train=y_train_bal,\n",
    "    X_test=X_test,\n",
    "    fit_params={\"validation_data\": (X_val, y_val)},\n",
    "    options={\"save\": False},\n",
    ")\n",
    "result_2 = job_2.result()\n",
    "print(\"Action status:\", result_2.get(\"status\"))\n",
    "print(\"Action message:\", result_2.get(\"message\"))\n",
    "print(\"QPU Time:\", result_2.get(\"metadata\"))\n",
    "qeec_pred_job_2 = np.array(result_2[\"data\"][\"predictions\"])\n",
    "_ = evaluate_predictions(qeec_pred_job_2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edd22b98-7ae6-4444-8fe5-7279e9233c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantum job status: DONE\n"
     ]
    }
   ],
   "source": [
    "status_2 = job_2.status()\n",
    "print(\"\\nQuantum job status:\", status_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94784a-3599-4911-ade9-4792b50c31bb",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "In this configuration, we increase to `num_learners = 60` and introduce adaptive regularization to manage sparsity more intuitively.\n",
    "\n",
    "- With `regularization = \"auto\"`, the optimizer automatically finds a suitable regularization strength that selects approximately `regularization_ratio * num_learners` weak learners for the final ensemble, rather than fixing the penalty manually. This provides a more convenient interface for managing the balance between sparsity and ensemble size.\n",
    "- `regularization_type = \"alpha\"` defines how the penalty is applied. Unlike `onsite`, which is unbounded `[0, ∞]`, `alpha` is bounded between `[0, 1]`, making it easier to tune and interpret. The parameter controls the trade-off between individual and pairwise penalties, offering a smoother configuration range.\n",
    "- `regularization_desired_ratio ≈ 0.82` specifies the target proportion of learners to keep active after regularization — here, around 82% of learners are retained, trimming the weakest 18% automatically.\n",
    "\n",
    "While adaptive regularization simplifies configuration and helps maintain a balanced ensemble, it does not necessarily guarantee better or more stable performance. The actual quality depends on selecting an appropriate regularization parameter, and fine-tuning it through cross-validation can be computationally expensive. The main advantage lies in improved usability and interpretability rather than direct accuracy gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "209eb51a-e44b-4f94-8975-269ec7e3d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem scale and regularization\n",
    "NUM_LEARNERS = 60\n",
    "REGULARIZATION = \"auto\"\n",
    "REGULARIZATION_TYPE = \"alpha\"\n",
    "REGULARIZATION_RATIO = 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb2aa9db-4427-48de-aae0-c5c1da1cb998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Submitting quantum-enhanced ensemble job --\n",
      "Action status: ok\n",
      "Action message: Classifier created, fitted, and predicted.\n",
      "Metadata: {'resource_usage': {'RUNNING: MAPPING': {'CPU_TIME': 1387.7451872825623}, 'RUNNING: WAITING_QPU': {'CPU_TIME': 95.41597843170166}, 'RUNNING: POST_PROCESSING': {'CPU_TIME': 171.78878355026245}, 'RUNNING: EXECUTING_QPU': {'QPU_TIME': 1146.5584812164307}}}\n",
      "Accuracy: 0.908\n",
      "Precision: 1.0\n",
      "Recall: 0.908\n",
      "F1: 0.9517819706498952\n"
     ]
    }
   ],
   "source": [
    "# ----- Quantum-enhanced ensemble on IBM hardware -----\n",
    "print(\"\\n-- Submitting quantum-enhanced ensemble job --\")\n",
    "job_3 = singularity.run(\n",
    "    action=\"create_fit_predict\",\n",
    "    name=\"grid_stability_qeec\",\n",
    "    quantum_classifier=\"QuantumEnhancedEnsembleClassifier\",\n",
    "    num_learners=NUM_LEARNERS,\n",
    "    regularization=REGULARIZATION,\n",
    "    regularization_type=REGULARIZATION_TYPE,\n",
    "    regularization_desired_ratio=REGULARIZATION_RATIO,\n",
    "    optimizer_options=optimizer_options,  # from Step 2\n",
    "    backend_name=backend,  # least-busy compatible backend\n",
    "    instance=IBM_INSTANCE_QUANTUM,\n",
    "    random_state=RANDOM_STATE,\n",
    "    X_train=X_train_bal,\n",
    "    y_train=y_train_bal,\n",
    "    X_test=X_test,\n",
    "    fit_params={\"validation_data\": (X_val, y_val)},\n",
    "    options={\"save\": False},\n",
    ")\n",
    "result_3 = job_3.result()\n",
    "print(\"Action status:\", result_3.get(\"status\"))\n",
    "print(\"Action message:\", result_3.get(\"message\"))\n",
    "print(\"Metadata:\", result_3.get(\"metadata\"))\n",
    "qeec_pred_job_3 = np.array(result_3[\"data\"][\"predictions\"])\n",
    "_ = evaluate_predictions(qeec_pred_job_3, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "299b36cb-0ed8-4af3-b431-a87daec04c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantum job status: DONE\n"
     ]
    }
   ],
   "source": [
    "status_3 = job_3.status()\n",
    "print(\"\\nQuantum job status:\", status_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b94af2",
   "metadata": {},
   "source": [
    "## Step 4: Post-process and return result in desired classical format\n",
    "\n",
    "We now post-process outputs from both the classical and quantum runs, converting them into a consistent format for downstream evaluation. This step compares predictive quality using standard metrics - accuracy, precision, recall, and F1 - and analyzes how ensemble width (`num_learners`) and sparsity control (`regularization`) influence both performance and computational behavior.\n",
    "\n",
    "The classical AdaBoost baseline provides a compact and stable reference for small-scale learning. It performs well with limited ensembles and negligible compute overhead, reflecting the strength of traditional boosting when the hypothesis space is still tractable. The quantum configurations (`qeec_pred_job_1`, `qeec_pred_job_2`, and `qeec_pred_job_3`) extend this baseline by embedding the ensemble-selection process within a variational quantum optimization loop. This allows the system to explore exponentially large subsets of learners simultaneously in superposition, addressing the combinatorial nature of ensemble selection more efficiently as scale increases.\n",
    "\n",
    "Results show that increasing `num_learners` from 10 to 30 improves recall and F1, confirming that a wider ensemble captures richer interactions among weak learners. The gain is sublinear on current hardware - each additional learner yields smaller accuracy increments - but the underlying scaling behavior remains favorable because the quantum optimizer can search broader configuration spaces without the exponential blow-up typical of classical subset selection. Regularization introduces additional nuance: a fixed λ=7 enforces consistent sparsity and stabilizes convergence, whereas adaptive α-regularization automatically tunes sparsity based on correlations between learners. This dynamic pruning often achieves slightly higher F1 for the same qubit width, balancing model complexity and generalization.\n",
    "\n",
    "When compared directly with the AdaBoost baseline, the smallest quantum configuration (L=10) reproduces similar accuracy, validating the hybrid pipeline’s correctness. At larger widths, quantum variants - especially with auto-regularization - begin to surpass the classical baseline modestly, showing improved recall and F1 without linear growth in computational cost. These improvements do not indicate immediate \"quantum advantage\" but rather **scaling efficiency**: the quantum optimizer maintains tractable performance as the ensemble expands, where a classical approach would face exponential growth in subset-selection complexity.\n",
    "\n",
    "In practice:\n",
    "- Use the **classical baseline** for quick validation and benchmarking on small datasets.\n",
    "- Apply **quantum ensembles** when model width or feature complexity grows—QAOA-based search scales more gracefully in those regimes.\n",
    "- Employ **adaptive α-regularization** to maintain sparsity and generalization without increasing circuit width.\n",
    "- Monitor QPU time and depth to balance quality gains against near-term hardware constraints.\n",
    "\n",
    "Together, these experiments show that quantum-optimized ensembles complement classical methods: they reproduce baseline accuracy at small scales while offering a path to efficient scaling on larger, combinatorial learning problems. As hardware improves, these scaling advantages are expected to compound, extending the feasible size and depth of ensemble-based models beyond what is classically practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1d610-f2e2-482c-af05-a882bb380528",
   "metadata": {},
   "source": [
    "### Evaluate metrics for each configuration\n",
    "\n",
    "We now evaluate all configurations - the classical AdaBoost baseline and the three quantum ensembles - using the `evaluate_predictions` helper to compute accuracy, precision, recall, and F1 on the same test set. This comparison clarifies how quantum optimization scales relative to the classical approach: at small widths, both perform similarly; as ensembles grow, the quantum method can explore larger hypothesis spaces more efficiently. The resulting table captures these trends in a consistent, quantitative form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee8cc821-8311-4c84-8f94-ffb2ec3facc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7893333333333333\n",
      "Precision: 1.0\n",
      "Recall: 0.7893333333333333\n",
      "F1: 0.8822652757078987\n",
      "Accuracy: 0.868\n",
      "Precision: 1.0\n",
      "Recall: 0.868\n",
      "F1: 0.9293361884368309\n",
      "Accuracy: 0.8946666666666667\n",
      "Precision: 1.0\n",
      "Recall: 0.8946666666666667\n",
      "F1: 0.944405348346235\n",
      "Accuracy: 0.908\n",
      "Precision: 1.0\n",
      "Recall: 0.908\n",
      "F1: 0.9517819706498952\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Config</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost (Classical)</td>\n",
       "      <td>0.789333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.789333</td>\n",
       "      <td>0.882265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QEEC L=10, reg=7</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.929336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QEEC L=30, reg=7</td>\n",
       "      <td>0.894667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.894667</td>\n",
       "      <td>0.944405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QEEC L=60, reg=auto (α=0.82)</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.951782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Config  Accuracy  Precision    Recall        F1\n",
       "0          AdaBoost (Classical)  0.789333        1.0  0.789333  0.882265\n",
       "1              QEEC L=10, reg=7  0.868000        1.0  0.868000  0.929336\n",
       "2              QEEC L=30, reg=7  0.894667        1.0  0.894667  0.944405\n",
       "3  QEEC L=60, reg=auto (α=0.82)  0.908000        1.0  0.908000  0.951782"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Classical baseline\n",
    "acc_b, prec_b, rec_b, f1_b = evaluate_predictions(baseline_pred, y_test)\n",
    "results.append(\n",
    "    {\n",
    "        \"Config\": \"AdaBoost (Classical)\",\n",
    "        \"Accuracy\": acc_b,\n",
    "        \"Precision\": prec_b,\n",
    "        \"Recall\": rec_b,\n",
    "        \"F1\": f1_b,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Quantum runs\n",
    "for label, preds in [\n",
    "    (\"QEEC L=10, reg=7\", qeec_pred_job_1),\n",
    "    (\"QEEC L=30, reg=7\", qeec_pred_job_2),\n",
    "    (f\"QEEC L=60, reg=auto (α={REGULARIZATION_RATIO})\", qeec_pred_job_3),\n",
    "]:\n",
    "    acc, prec, rec, f1 = evaluate_predictions(preds, y_test)\n",
    "    results.append(\n",
    "        {\n",
    "            \"Config\": label,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": prec,\n",
    "            \"Recall\": rec,\n",
    "            \"F1\": f1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cecf6-2c87-4356-b9be-26f4346c194f",
   "metadata": {},
   "source": "### 学習器数の増加\nここでは `num_learners` を10から30に増加させ、`regularization = 7` は維持します。\n\n- 学習器を増やすと仮説空間が拡大し、モデルがより微妙なパターンを捉えられるようになり、F1がわずかに向上する可能性があります。\n- ほとんどの場合、10個と30個の学習器間の実行時間の差は大きくなく、回路幅の増加が実行コストを大幅には増加させないことを示しています。\n- 品質の向上は依然として*収穫逓減曲線*に従います。アンサンブルが成長するにつれて初期の改善が現れますが、追加の学習器が提供する新しい情報が減少するにつれてプラトーに達します。\n\nこの実験は品質と効率のトレードオフを明らかにします。アンサンブル幅を増加させると、バックエンドとトランスパイレーション条件に応じて、大きな実行時間のペナルティなしにわずかな精度向上が得られる場合があります。"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f15c5fb-2450-4671-9bc2-471043414df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image src=\"../docs/images/tutorials/sml-classification/extracted-outputs/0f15c5fb-2450-4671-9bc2-471043414df2-0.avif\" alt=\"Output of the previous code cell\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(df_results))\n",
    "width = 0.35\n",
    "plt.figure(figsize=(7.6, 4.6))\n",
    "plt.bar(x - width / 2, df_results[\"Accuracy\"], width=width, label=\"Accuracy\")\n",
    "plt.bar(x + width / 2, df_results[\"F1\"], width=width, label=\"F1\")\n",
    "plt.xticks(x, df_results[\"Config\"], rotation=10)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Classical vs Quantum ensemble performance\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745cd406-5166-4eb9-8506-0eedd71e9b79",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The plot confirms the expected scaling pattern. The classical AdaBoost performs strongly for smaller ensembles but becomes increasingly costly to scale as the number of weak learners grows, because its subset-selection problem expands combinatorially. The quantum-enhanced models replicate classical accuracy at low widths and begin to surpass it as ensemble size increases, especially under adaptive α-regularization. This reflects the quantum optimizer’s ability to sample and evaluate many candidate subsets in parallel through superposition, maintaining tractable search even at higher widths. While current hardware overhead offsets some of the theoretical gains, the trend illustrates the scaling efficiency advantage of the quantum formulation. In practical terms, the classical method remains preferable for lightweight benchmarks, while quantum-enhanced ensembles become advantageous as model dimensionality and ensemble size expand, offering better trade-offs between accuracy, generalization, and computational growth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27afb68-f959-4f4a-910c-0aadafb7e18e",
   "metadata": {},
   "source": [
    "## Appendix: Scaling benefits and enhancements\n",
    "\n",
    "The scalability advantage of the `QuantumEnhancedEnsembleClassifier` arises from how the ensemble-selection process maps to quantum optimization.\n",
    "Classical ensemble learning methods, such as AdaBoost or random forests, become computationally expensive as the number of weak learners increases because selecting the optimal subset is a combinatorial problem that scales exponentially.\n",
    "\n",
    "In contrast, the quantum formulation — implemented here via the Quantum Approximate Optimization Algorithm (QAOA) — can explore these exponentially large search spaces more efficiently by evaluating multiple configurations in superposition.\n",
    "As a result, the training time does not grow significantly with the number of learners, allowing the model to remain efficient even as ensemble width increases.\n",
    "\n",
    "While current hardware introduces some noise and depth limitations, this workflow demonstrates a near-term hybrid approach where classical and quantum components cooperate: the quantum optimizer provides a better initialization landscape for the classical loop, improving convergence and final model quality.\n",
    "As quantum processors evolve, these scalability benefits are expected to extend to larger datasets, broader ensembles, and deeper circuit depths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee41a301",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [Introduction to Qiskit Functions](/docs/guides/functions)\n",
    "2. [Multiverse Computing Singularity Machine Learning](/docs/guides/multiverse-computing-singularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5785c",
   "metadata": {},
   "source": [
    "## Tutorial survey\n",
    "\n",
    "Please take a minute to provide feedback on this tutorial. Your insights will help us improve our content offerings and user experience.\n",
    "\n",
    "[Link to survey](https://your.feedback.ibm.com/jfe/form/SV_3BLFkNVEuh0QBWm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3"
  },
  "colab": {
   "cell_execution_strategy": "setup"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
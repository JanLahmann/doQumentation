{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: install Qiskit (runs automatically in Colab, no-op in Binder)\n",
    "!pip install -q qiskit qiskit-aer qiskit-ibm-runtime pylatexenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional dependencies for this notebook\n",
    "!pip install -q imbalanced-learn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d1e3ec",
   "metadata": {},
   "source": "# Classification hybride améliorée par le quantique avec ensemble (workflow de stabilité du réseau)\n\n*Estimation d'utilisation : 20 minutes de temps QPU pour chaque tâche sur un processeur Eagle r3. (REMARQUE : il s'agit uniquement d'une estimation. Votre temps d'exécution peut varier.)*\n## Contexte\nCe tutoriel présente un workflow hybride quantique-classique qui améliore un ensemble classique grâce à une étape d'optimisation quantique. En utilisant la fonction « Singularity Machine Learning – Classification » de Multiverse Computing (une fonction Qiskit), nous entraînons un pool d'apprenants conventionnels (par exemple, arbres de décision, k-NN, régression logistique) puis affinons ce pool avec une couche quantique pour améliorer la diversité et la généralisation. L'objectif est pratique : sur une tâche réelle de prédiction de stabilité du réseau, nous comparons une base classique solide avec une alternative optimisée par le quantique sur les mêmes partitions de données, afin que vous puissiez voir où l'étape quantique apporte un bénéfice et ce qu'elle coûte.\n\nPourquoi c'est important : sélectionner un bon sous-ensemble parmi de nombreux apprenants faibles est un problème combinatoire qui croît rapidement avec la taille de l'ensemble. Les heuristiques classiques comme le boosting, le bagging et le stacking fonctionnent bien à des échelles modérées mais peuvent avoir du mal à explorer efficacement de grandes bibliothèques redondantes de modèles. La fonction intègre des algorithmes quantiques — spécifiquement QAOA (et optionnellement VQE dans d'autres configurations) — pour parcourir cet espace plus efficacement après l'entraînement des apprenants classiques, augmentant ainsi les chances de trouver un sous-ensemble compact et diversifié qui généralise mieux.\n\nPoint crucial : l'échelle des données n'est pas limitée par les qubits. Le travail intensif sur les données — prétraitement, entraînement du pool d'apprenants et évaluation — reste classique et peut traiter des millions d'exemples. Les qubits ne déterminent que la taille de l'ensemble utilisée dans l'étape de sélection quantique. Ce découplage est ce qui rend l'approche viable sur le matériel actuel : vous conservez les workflows scikit-learn familiers pour les données et l'entraînement des modèles tout en appelant l'étape quantique via une interface d'action propre dans les fonctions Qiskit.\n\nEn pratique, bien que différents types d'apprenants puissent être fournis à l'ensemble (par exemple, arbres de décision, régression logistique ou k-NN), les arbres de décision tendent à donner les meilleurs résultats. L'optimiseur favorise systématiquement les membres d'ensemble les plus performants — lorsque des apprenants hétérogènes sont fournis, les modèles plus faibles comme les régresseurs linéaires sont généralement élagués au profit de modèles plus expressifs comme les arbres de décision.\n\nCe que vous allez faire ici : préparer et équilibrer le jeu de données de stabilité du réseau ; établir une base classique AdaBoost ; exécuter plusieurs configurations quantiques variant la largeur de l'ensemble et la régularisation ; exécuter sur des simulateurs ou QPU IBM&reg; via Qiskit Serverless ; et comparer la précision, la précision (precision), le rappel et le F1 à travers toutes les exécutions. Tout au long, vous utiliserez le motif d'action de la fonction (`create`, `fit`, `predict`, `fit_predict`, `create_fit_predict`) et les contrôles clés :\n- Types de régularisation : `onsite` (λ) pour la parcimonie directe et `alpha` pour un compromis basé sur un ratio entre les termes d'interaction et les termes onsite\n- Auto-régularisation : définissez `regularization=\"auto\"` avec un ratio de sélection cible pour adapter la parcimonie automatiquement\n- Options de l'optimiseur : simulateur versus QPU, répétitions, optimiseur classique et ses options, profondeur de transpilation, et paramètres d'exécution du sampler/estimator\n\nLes benchmarks dans la documentation montrent que la précision s'améliore à mesure que le nombre d'apprenants (qubits) augmente sur les problèmes difficiles, le classificateur quantique égalant ou dépassant un ensemble classique comparable. Dans ce tutoriel, vous reproduirez le workflow de bout en bout et examinerez quand l'augmentation de la largeur de l'ensemble ou le passage à la régularisation adaptative produit un meilleur F1 avec une utilisation raisonnable des ressources. Le résultat est une vision fondée de la façon dont une étape d'optimisation quantique peut compléter, plutôt que remplacer, l'apprentissage par ensemble classique dans des applications réelles.\n## Prérequis\nAvant de commencer ce tutoriel, assurez-vous que les paquets suivants sont installés dans votre environnement Python :\n\n- `qiskit[visualization]~=2.1.0`\n- `qiskit-serverless~=0.24.0`\n- `qiskit-ibm-runtime v0.40.1`\n- `qiskit-ibm-catalog~=0.8.0`\n- `scikit-learn==1.5.2`\n- `pandas>=2.0.0,<3.0.0`\n- `imbalanced-learn~=0.12.3`\n## Configuration\nDans cette section, nous initialisons le client Qiskit Serverless et chargeons la fonction Singularity Machine Learning – Classification fournie par Multiverse Computing.\nAvec Qiskit Serverless, vous pouvez exécuter des workflows hybrides quantiques-classiques sur l'infrastructure cloud gérée par IBM sans vous soucier de la gestion des ressources.\nVous aurez besoin d'une clé API IBM Quantum Platform et du nom de votre ressource cloud (CRN) pour vous authentifier et accéder aux fonctions Qiskit.\n### Téléchargement du jeu de données\nPour exécuter ce tutoriel, nous utilisons un **jeu de données de classification de stabilité du réseau** prétraité contenant des relevés de capteurs de systèmes électriques étiquetés.\nLa cellule suivante crée automatiquement la structure de dossiers requise et télécharge les fichiers d'entraînement et de test directement dans votre environnement à l'aide de `wget`.\nSi vous disposez déjà de ces fichiers localement, cette étape les écrasera en toute sécurité pour garantir la cohérence des versions."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6f69b77",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "8bf80006",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "55b94021",
   "metadata": {},
   "source": ""
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7db2e559",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "d4fe7ee1-21ce-445c-b151-598cd4cf9227",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a32efb3-a425-4c02-804b-65029ecffb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_tutorial/grid_ 100%[===================>] 612.94K  --.-KB/s    in 0.01s   \n",
      "data_tutorial/grid_ 100%[===================>] 108.19K  --.-KB/s    in 0.006s  \n",
      "Dataset files downloaded:\n",
      "-rw-r--r-- 1 coder coder 109K Nov  8 18:50 data_tutorial/grid_stability/test.csv\n",
      "-rw-r--r-- 1 coder coder 613K Nov  8 18:50 data_tutorial/grid_stability/train.csv\n"
     ]
    }
   ],
   "source": [
    "## Download dataset for Grid Stability Classification\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "!mkdir -p data_tutorial/grid_stability\n",
    "\n",
    "# Download the training and test sets from the official Qiskit documentation repo\n",
    "!wget -q --show-progress -O data_tutorial/grid_stability/train.csv \\\n",
    "  https://raw.githubusercontent.com/Qiskit/documentation/main/datasets/tutorials/grid_stability/train.csv\n",
    "\n",
    "!wget -q --show-progress -O data_tutorial/grid_stability/test.csv \\\n",
    "  https://raw.githubusercontent.com/Qiskit/documentation/main/datasets/tutorials/grid_stability/test.csv\n",
    "\n",
    "# Check the files have been downloaded\n",
    "!echo \"Dataset files downloaded:\"\n",
    "!ls -lh data_tutorial/grid_stability/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9aa84f-ab37-412c-b056-7043b73380fa",
   "metadata": {},
   "source": [
    "### Import required packages\n",
    "\n",
    "In this section, we import all Python packages and Qiskit modules used throughout the tutorial.\n",
    "These include core scientific libraries for data handling and model evaluation - such as `NumPy`, `pandas`, and `scikit-learn` - along with visualization tools and Qiskit components for running the quantum-enhanced model.\n",
    "We also import the `QiskitRuntimeService` and `QiskitFunctionsCatalog` to connect with IBM Quantum&reg; services and access the Singularity Machine Learning function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c654f5-8355-4f67-b79d-c2b1c29ccc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from qiskit_ibm_catalog import QiskitFunctionsCatalog\n",
    "from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b389b34-02e8-4a63-ae24-0bf348647b48",
   "metadata": {},
   "source": "### Importation des paquets requis\nDans cette section, nous importons tous les paquets Python et modules Qiskit utilisés tout au long du tutoriel.\nCeux-ci incluent les bibliothèques scientifiques essentielles pour la manipulation des données et l'évaluation des modèles — comme `NumPy`, `pandas` et `scikit-learn` — ainsi que les outils de visualisation et les composants Qiskit pour l'exécution du modèle amélioré par le quantique.\nNous importons également `QiskitRuntimeService` et `QiskitFunctionsCatalog` pour nous connecter aux services IBM Quantum&reg; et accéder à la fonction Singularity Machine Learning."
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a68bf7df-914c-4b2a-827f-657578503750",
   "metadata": {},
   "outputs": [],
   "source": [
    "IBM_TOKEN = \"\"\n",
    "IBM_INSTANCE_TEST = \"\"\n",
    "IBM_INSTANCE_QUANTUM = \"\"\n",
    "FUNCTION_NAME = \"multiverse/singularity\"\n",
    "RANDOM_STATE: int = 123\n",
    "TRAIN_PATH = \"data_tutorial/grid_stability/train.csv\"\n",
    "TEST_PATH = \"data_tutorial/grid_stability/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4709dc1c-b380-49f1-95c7-89197aa5e147",
   "metadata": {},
   "source": "### Définition des variables constantes"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc380c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to IBM Qiskit Serverless and loaded the Singularity function.\n",
      "Catalog: <QiskitFunctionsCatalog>\n",
      "Singularity function: QiskitFunction(multiverse/singularity)\n"
     ]
    }
   ],
   "source": [
    "service = QiskitRuntimeService(\n",
    "    token=IBM_TOKEN,\n",
    "    channel=\"ibm_quantum_platform\",\n",
    "    instance=IBM_INSTANCE_QUANTUM,\n",
    ")\n",
    "\n",
    "backend = service.least_busy()\n",
    "catalog = QiskitFunctionsCatalog(\n",
    "    token=IBM_TOKEN,\n",
    "    instance=IBM_INSTANCE_TEST,\n",
    "    channel=\"ibm_quantum_platform\",\n",
    ")\n",
    "singularity = catalog.load(FUNCTION_NAME)\n",
    "print(\n",
    "    \"Successfully connected to IBM Qiskit Serverless and loaded the Singularity function.\"\n",
    ")\n",
    "print(\"Catalog:\", catalog)\n",
    "print(\"Singularity function:\", singularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d6a559-118a-4aa9-874d-9c009b5da60c",
   "metadata": {},
   "source": "### Connexion à IBM Quantum et chargement de la fonction Singularity\nEnsuite, nous nous authentifions auprès des services IBM Quantum et chargeons la fonction Singularity Machine Learning – Classification depuis le catalogue des fonctions Qiskit.\nLe `QiskitRuntimeService` établit une connexion sécurisée à IBM Quantum Platform en utilisant votre jeton API et le CRN de votre instance, permettant l'accès aux backends quantiques.\nLe `QiskitFunctionsCatalog` est ensuite utilisé pour récupérer la fonction Singularity par son nom (`\"multiverse/singularity\"`), nous permettant de l'appeler ultérieurement pour le calcul hybride quantique-classique.\nSi la configuration est réussie, vous verrez un message de confirmation indiquant que la fonction a été chargée correctement."
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46bc841e-7365-4508-b6bf-ae57db6050e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load data from the given path to X and y arrays.\"\"\"\n",
    "    df: pd.DataFrame = pd.read_csv(data_path)\n",
    "    return df.iloc[:, :-1].values, df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "def evaluate_predictions(predictions, y_true):\n",
    "    \"\"\"Compute and print accuracy, precision, recall, and F1 score.\"\"\"\n",
    "    accuracy = accuracy_score(y_true, predictions)\n",
    "    precision = precision_score(y_true, predictions)\n",
    "    recall = recall_score(y_true, predictions)\n",
    "    f1 = f1_score(y_true, predictions)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1:\", f1)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "988ee237",
   "metadata": {},
   "source": [
    "## Step 1: Map classical inputs to a quantum problem\n",
    "\n",
    "We begin by preparing the dataset for hybrid quantum–classical experimentation. The goal of this step is to convert the raw grid-stability data into balanced training, validation, and test splits that can be used consistently by both classical and quantum workflows. Maintaining identical splits ensures that later performance comparisons are fair and reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c084cde-a5cf-4661-a00c-aa243c8b0e44",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing\n",
    "\n",
    "We first load the training and test CSV files, create a validation split, and balance the dataset using random over-sampling. Balancing prevents bias toward the majority class and provides a more stable learning signal for both classical and quantum ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db0e914-a8f2-4a04-bec7-c15bac8104b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "  X_train_bal: (5104, 12)\n",
      "  y_train_bal: (5104,)\n",
      "  X_val: (850, 12)\n",
      "  y_val: (850,)\n",
      "  X_test: (750, 12)\n",
      "  y_test: (750,)\n"
     ]
    }
   ],
   "source": [
    "# Load and upload the data\n",
    "X_train, y_train = load_data(TRAIN_PATH)\n",
    "X_test, y_test = load_data(TEST_PATH)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Balance the dataset through over-sampling of the positive class\n",
    "ros = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "X_train_bal, y_train_bal = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_train_bal:\", X_train_bal.shape)\n",
    "print(\"  y_train_bal:\", y_train_bal.shape)\n",
    "print(\"  X_val:\", X_val.shape)\n",
    "print(\"  y_val:\", y_val.shape)\n",
    "print(\"  X_test:\", X_test.shape)\n",
    "print(\"  y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c214fd-09a8-4fa5-ab83-b0c71ad615d4",
   "metadata": {},
   "source": "### Définition des fonctions utilitaires\nAvant d'exécuter les expériences principales, nous définissons quelques petites fonctions utilitaires qui simplifient le chargement des données et l'évaluation des modèles.\n- `load_data()` lit les fichiers CSV d'entrée sous forme de tableaux NumPy, séparant les caractéristiques et les étiquettes pour la compatibilité avec `scikit-learn` et les workflows quantiques.\n- `evaluate_predictions()` calcule les métriques de performance clés — précision (accuracy), précision (precision), rappel et score F1 — et rapporte optionnellement le temps d'exécution si des informations de chronométrage sont fournies.\n\nCes fonctions utilitaires simplifient les opérations répétées plus loin dans le notebook et garantissent un reporting cohérent des métriques entre les classificateurs classiques et quantiques."
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38ecd608-35b4-4e58-8a16-85ef98bf024a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical AdaBoost baseline:\n",
      "Accuracy: 0.7893333333333333\n",
      "Precision: 1.0\n",
      "Recall: 0.7893333333333333\n",
      "F1: 0.8822652757078987\n"
     ]
    }
   ],
   "source": [
    "# ----- Classical baseline: AdaBoost -----\n",
    "baseline = AdaBoostClassifier(n_estimators=60, random_state=RANDOM_STATE)\n",
    "baseline.fit(X_train_bal, y_train_bal)\n",
    "baseline_pred = baseline.predict(X_test)\n",
    "print(\"Classical AdaBoost baseline:\")\n",
    "_ = evaluate_predictions(baseline_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f36e3",
   "metadata": {},
   "source": "## Étape 1 : Transformer les entrées classiques en un problème quantique\nNous commençons par préparer le jeu de données pour l'expérimentation hybride quantique-classique. L'objectif de cette étape est de convertir les données brutes de stabilité du réseau en partitions d'entraînement, de validation et de test équilibrées qui peuvent être utilisées de manière cohérente par les workflows classiques et quantiques. Le maintien de partitions identiques garantit que les comparaisons de performance ultérieures sont équitables et reproductibles.\n### Chargement et prétraitement des données\nNous chargeons d'abord les fichiers CSV d'entraînement et de test, créons une partition de validation, et équilibrons le jeu de données par sur-échantillonnage aléatoire. L'équilibrage prévient le biais en faveur de la classe majoritaire et fournit un signal d'apprentissage plus stable pour les modèles d'ensemble classiques et quantiques."
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c0d3f9-691b-449d-83ca-e6bfce2d6b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured hardware optimization profile:\n",
      "  simulator: False\n",
      "  num_solutions: 100000\n",
      "  reps: 3\n",
      "  optimization_level: 3\n",
      "  num_transpiler_runs: 30\n",
      "  classical_optimizer: COBYLA\n",
      "  classical_optimizer_options: {'maxiter': 60}\n",
      "  estimator_options: None\n",
      "  sampler_options: None\n"
     ]
    }
   ],
   "source": [
    "# QAOA / runtime configuration for best results on hardware\n",
    "optimizer_options = {\n",
    "    \"simulator\": False,  # set True to test locally without QPU\n",
    "    \"num_solutions\": 100_000,  # broaden search over candidate ensembles\n",
    "    \"reps\": 3,  # QAOA depth (circuit layers)\n",
    "    \"optimization_level\": 3,  # transpilation effort\n",
    "    \"num_transpiler_runs\": 30,  # explore multiple layouts\n",
    "    \"classical_optimizer\": \"COBYLA\",  # robust default for this landscape\n",
    "    \"classical_optimizer_options\": {\n",
    "        \"maxiter\": 60  # practical convergence budget\n",
    "    },\n",
    "    # You can pass backend-specific options; leaving None uses least-busy routing\n",
    "    \"estimator_options\": None,\n",
    "    \"sampler_options\": None,\n",
    "}\n",
    "\n",
    "print(\"Configured hardware optimization profile:\")\n",
    "for key, value in optimizer_options.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4d480b3",
   "metadata": {},
   "source": [
    "## Step 3: Execute using Qiskit primitives\n",
    "\n",
    "We now execute the full workflow using the Singularity function’s `create_fit_predict` action to train, optimize, and evaluate the `QuantumEnhancedEnsembleClassifier` end-to-end on IBM infrastructure. The function builds the ensemble, applies quantum optimization through Qiskit primitives, and returns both predictions and job metadata (including runtime and resource usage). The classical data split from Step 1 is reused for reproducibility, with validation data passed through `fit_params` so the optimization can tune hyperparameters internally while keeping the held-out test set untouched.\n",
    "\n",
    "In this step, we explore several configurations of the quantum ensemble to understand how key parameters - specifically `num_learners` and `regularization` - affect both result quality and QPU usage.\n",
    "- `num_learners` determines the ensemble width (and implicitly, the number of qubits), influencing the model’s capacity and computational cost.\n",
    "- `regularization` controls sparsity and overfitting, shaping how many learners remain active after optimization.\n",
    "\n",
    "By varying these parameters, we can see how ensemble width and regularization interact: increasing width typically improves F1 but costs more QPU time, while stronger or adaptive regularization can improve generalization at roughly the same hardware footprint. The next subsections walk through three representative configurations to illustrate these effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da41d4d-f06d-4b67-a8ee-b07fc0289558",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "This configuration uses `num_learners = 10` and `regularization = 7`.\n",
    "\n",
    "- `num_learners` controls the ensemble width — effectively the number of weak learners combined and, on quantum hardware, the **number of qubits required**. A larger value expands the combinatorial search space and can improve accuracy and recall, but also increases circuit width, compilation time, and overall QPU usage.\n",
    "- `regularization` sets the penalty strength for including additional learners. With the default \"onsite\" regularization, higher values enforce stronger sparsity (fewer learners kept), while lower values allow more complex ensembles.\n",
    "\n",
    "This setup provides a low-cost baseline, showing how a small ensemble behaves before scaling width or tuning sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e702c986-fd7f-4ea1-a93c-8cd5ef0c4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem scale and regularization\n",
    "NUM_LEARNERS = 10\n",
    "REGULARIZATION = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06c261c1-a3c1-42c9-a522-63f3fc01a970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Submitting quantum-enhanced ensemble job --\n",
      "Action status: ok\n",
      "Action message: Classifier created, fitted, and predicted.\n",
      "Metadata: {'resource_usage': {'RUNNING: MAPPING': {'CPU_TIME': 267.05158376693726}, 'RUNNING: WAITING_QPU': {'CPU_TIME': 3336.8785166740417}, 'RUNNING: POST_PROCESSING': {'CPU_TIME': 152.4274561405182}, 'RUNNING: EXECUTING_QPU': {'QPU_TIME': 1550.1889700889587}}}\n",
      "Accuracy: 0.868\n",
      "Precision: 1.0\n",
      "Recall: 0.868\n",
      "F1: 0.9293361884368309\n"
     ]
    }
   ],
   "source": [
    "# ----- Quantum-enhanced ensemble on IBM hardware -----\n",
    "print(\"\\n-- Submitting quantum-enhanced ensemble job --\")\n",
    "job_1 = singularity.run(\n",
    "    action=\"create_fit_predict\",\n",
    "    name=\"grid_stability_qeec\",\n",
    "    quantum_classifier=\"QuantumEnhancedEnsembleClassifier\",\n",
    "    num_learners=NUM_LEARNERS,\n",
    "    regularization=REGULARIZATION,\n",
    "    optimizer_options=optimizer_options,  # from Step 2\n",
    "    backend_name=backend,  # least-busy compatible backend\n",
    "    instance=IBM_INSTANCE_QUANTUM,\n",
    "    random_state=RANDOM_STATE,\n",
    "    X_train=X_train_bal,\n",
    "    y_train=y_train_bal,\n",
    "    X_test=X_test,\n",
    "    fit_params={\"validation_data\": (X_val, y_val)},\n",
    "    options={\"save\": False},\n",
    ")\n",
    "result_1 = job_1.result()\n",
    "print(\"Action status:\", result_1.get(\"status\"))\n",
    "print(\"Action message:\", result_1.get(\"message\"))\n",
    "print(\"Metadata:\", result_1.get(\"metadata\"))\n",
    "qeec_pred_job_1 = np.array(result_1[\"data\"][\"predictions\"])\n",
    "_ = evaluate_predictions(qeec_pred_job_1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf0a3ae-b5b8-43d7-b72d-5a9cf6c61060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantum job status: DONE\n"
     ]
    }
   ],
   "source": [
    "status_1 = job_1.status()\n",
    "print(\"\\nQuantum job status:\", status_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62b414-db77-45aa-bf2c-a8537fd7eba0",
   "metadata": {},
   "source": "## Étape 2 : Optimiser le problème pour l'exécution sur matériel quantique\nLa tâche de sélection d'ensemble est formulée comme un problème d'optimisation combinatoire où chaque apprenant faible est une variable de décision binaire, et l'objectif équilibre la précision avec la parcimonie à travers un terme de régularisation. Le `QuantumEnhancedEnsembleClassifier` résout cela avec QAOA sur du matériel IBM, tout en permettant l'exploration basée sur simulateur. Les `optimizer_options` contrôlent la boucle hybride : `simulator=False` dirige les circuits vers le QPU sélectionné, `num_solutions` augmente la largeur de recherche, et `classical_optimizer_options` (pour l'optimiseur classique interne) régissent la convergence ; des valeurs autour de 60 itérations offrent un bon équilibre entre qualité et temps d'exécution. Les options d'exécution — comme une profondeur de circuit modérée (`reps`) et un effort de transpilation standard — contribuent à garantir des performances robustes sur les différents appareils. La configuration ci-dessous est le profil « meilleurs résultats » que nous utiliserons pour les exécutions sur matériel ; vous pouvez également créer une variante purement simulée en basculant `simulator=True` pour tester le workflow sans consommer de temps QPU."
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5fe64b0-2713-4b65-b768-10fa5d8dbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem scale and regularization\n",
    "NUM_LEARNERS = 30\n",
    "REGULARIZATION = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18a22fed-41c2-4407-88d4-02fbd32f3320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Submitting quantum-enhanced ensemble job --\n",
      "Action status: ok\n",
      "Action message: Classifier created, fitted, and predicted.\n",
      "QPU Time: {'resource_usage': {'RUNNING: MAPPING': {'CPU_TIME': 680.2116754055023}, 'RUNNING: WAITING_QPU': {'CPU_TIME': 80.80395102500916}, 'RUNNING: POST_PROCESSING': {'CPU_TIME': 154.4466371536255}, 'RUNNING: EXECUTING_QPU': {'QPU_TIME': 1095.822762966156}}}\n",
      "Accuracy: 0.8946666666666667\n",
      "Precision: 1.0\n",
      "Recall: 0.8946666666666667\n",
      "F1: 0.944405348346235\n"
     ]
    }
   ],
   "source": [
    "# ----- Quantum-enhanced ensemble on IBM hardware -----\n",
    "print(\"\\n-- Submitting quantum-enhanced ensemble job --\")\n",
    "job_2 = singularity.run(\n",
    "    action=\"create_fit_predict\",\n",
    "    name=\"grid_stability_qeec\",\n",
    "    quantum_classifier=\"QuantumEnhancedEnsembleClassifier\",\n",
    "    num_learners=NUM_LEARNERS,\n",
    "    regularization=REGULARIZATION,\n",
    "    optimizer_options=optimizer_options,  # from Step 2\n",
    "    backend_name=backend,  # least-busy compatible backend\n",
    "    instance=IBM_INSTANCE_QUANTUM,\n",
    "    random_state=RANDOM_STATE,\n",
    "    X_train=X_train_bal,\n",
    "    y_train=y_train_bal,\n",
    "    X_test=X_test,\n",
    "    fit_params={\"validation_data\": (X_val, y_val)},\n",
    "    options={\"save\": False},\n",
    ")\n",
    "result_2 = job_2.result()\n",
    "print(\"Action status:\", result_2.get(\"status\"))\n",
    "print(\"Action message:\", result_2.get(\"message\"))\n",
    "print(\"QPU Time:\", result_2.get(\"metadata\"))\n",
    "qeec_pred_job_2 = np.array(result_2[\"data\"][\"predictions\"])\n",
    "_ = evaluate_predictions(qeec_pred_job_2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edd22b98-7ae6-4444-8fe5-7279e9233c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantum job status: DONE\n"
     ]
    }
   ],
   "source": [
    "status_2 = job_2.status()\n",
    "print(\"\\nQuantum job status:\", status_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94784a-3599-4911-ade9-4792b50c31bb",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "In this configuration, we increase to `num_learners = 60` and introduce adaptive regularization to manage sparsity more intuitively.\n",
    "\n",
    "- With `regularization = \"auto\"`, the optimizer automatically finds a suitable regularization strength that selects approximately `regularization_ratio * num_learners` weak learners for the final ensemble, rather than fixing the penalty manually. This provides a more convenient interface for managing the balance between sparsity and ensemble size.\n",
    "- `regularization_type = \"alpha\"` defines how the penalty is applied. Unlike `onsite`, which is unbounded `[0, ∞]`, `alpha` is bounded between `[0, 1]`, making it easier to tune and interpret. The parameter controls the trade-off between individual and pairwise penalties, offering a smoother configuration range.\n",
    "- `regularization_desired_ratio ≈ 0.82` specifies the target proportion of learners to keep active after regularization — here, around 82% of learners are retained, trimming the weakest 18% automatically.\n",
    "\n",
    "While adaptive regularization simplifies configuration and helps maintain a balanced ensemble, it does not necessarily guarantee better or more stable performance. The actual quality depends on selecting an appropriate regularization parameter, and fine-tuning it through cross-validation can be computationally expensive. The main advantage lies in improved usability and interpretability rather than direct accuracy gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "209eb51a-e44b-4f94-8975-269ec7e3d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem scale and regularization\n",
    "NUM_LEARNERS = 60\n",
    "REGULARIZATION = \"auto\"\n",
    "REGULARIZATION_TYPE = \"alpha\"\n",
    "REGULARIZATION_RATIO = 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb2aa9db-4427-48de-aae0-c5c1da1cb998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Submitting quantum-enhanced ensemble job --\n",
      "Action status: ok\n",
      "Action message: Classifier created, fitted, and predicted.\n",
      "Metadata: {'resource_usage': {'RUNNING: MAPPING': {'CPU_TIME': 1387.7451872825623}, 'RUNNING: WAITING_QPU': {'CPU_TIME': 95.41597843170166}, 'RUNNING: POST_PROCESSING': {'CPU_TIME': 171.78878355026245}, 'RUNNING: EXECUTING_QPU': {'QPU_TIME': 1146.5584812164307}}}\n",
      "Accuracy: 0.908\n",
      "Precision: 1.0\n",
      "Recall: 0.908\n",
      "F1: 0.9517819706498952\n"
     ]
    }
   ],
   "source": [
    "# ----- Quantum-enhanced ensemble on IBM hardware -----\n",
    "print(\"\\n-- Submitting quantum-enhanced ensemble job --\")\n",
    "job_3 = singularity.run(\n",
    "    action=\"create_fit_predict\",\n",
    "    name=\"grid_stability_qeec\",\n",
    "    quantum_classifier=\"QuantumEnhancedEnsembleClassifier\",\n",
    "    num_learners=NUM_LEARNERS,\n",
    "    regularization=REGULARIZATION,\n",
    "    regularization_type=REGULARIZATION_TYPE,\n",
    "    regularization_desired_ratio=REGULARIZATION_RATIO,\n",
    "    optimizer_options=optimizer_options,  # from Step 2\n",
    "    backend_name=backend,  # least-busy compatible backend\n",
    "    instance=IBM_INSTANCE_QUANTUM,\n",
    "    random_state=RANDOM_STATE,\n",
    "    X_train=X_train_bal,\n",
    "    y_train=y_train_bal,\n",
    "    X_test=X_test,\n",
    "    fit_params={\"validation_data\": (X_val, y_val)},\n",
    "    options={\"save\": False},\n",
    ")\n",
    "result_3 = job_3.result()\n",
    "print(\"Action status:\", result_3.get(\"status\"))\n",
    "print(\"Action message:\", result_3.get(\"message\"))\n",
    "print(\"Metadata:\", result_3.get(\"metadata\"))\n",
    "qeec_pred_job_3 = np.array(result_3[\"data\"][\"predictions\"])\n",
    "_ = evaluate_predictions(qeec_pred_job_3, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "299b36cb-0ed8-4af3-b431-a87daec04c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantum job status: DONE\n"
     ]
    }
   ],
   "source": [
    "status_3 = job_3.status()\n",
    "print(\"\\nQuantum job status:\", status_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b94af2",
   "metadata": {},
   "source": [
    "## Step 4: Post-process and return result in desired classical format\n",
    "\n",
    "We now post-process outputs from both the classical and quantum runs, converting them into a consistent format for downstream evaluation. This step compares predictive quality using standard metrics - accuracy, precision, recall, and F1 - and analyzes how ensemble width (`num_learners`) and sparsity control (`regularization`) influence both performance and computational behavior.\n",
    "\n",
    "The classical AdaBoost baseline provides a compact and stable reference for small-scale learning. It performs well with limited ensembles and negligible compute overhead, reflecting the strength of traditional boosting when the hypothesis space is still tractable. The quantum configurations (`qeec_pred_job_1`, `qeec_pred_job_2`, and `qeec_pred_job_3`) extend this baseline by embedding the ensemble-selection process within a variational quantum optimization loop. This allows the system to explore exponentially large subsets of learners simultaneously in superposition, addressing the combinatorial nature of ensemble selection more efficiently as scale increases.\n",
    "\n",
    "Results show that increasing `num_learners` from 10 to 30 improves recall and F1, confirming that a wider ensemble captures richer interactions among weak learners. The gain is sublinear on current hardware - each additional learner yields smaller accuracy increments - but the underlying scaling behavior remains favorable because the quantum optimizer can search broader configuration spaces without the exponential blow-up typical of classical subset selection. Regularization introduces additional nuance: a fixed λ=7 enforces consistent sparsity and stabilizes convergence, whereas adaptive α-regularization automatically tunes sparsity based on correlations between learners. This dynamic pruning often achieves slightly higher F1 for the same qubit width, balancing model complexity and generalization.\n",
    "\n",
    "When compared directly with the AdaBoost baseline, the smallest quantum configuration (L=10) reproduces similar accuracy, validating the hybrid pipeline’s correctness. At larger widths, quantum variants - especially with auto-regularization - begin to surpass the classical baseline modestly, showing improved recall and F1 without linear growth in computational cost. These improvements do not indicate immediate \"quantum advantage\" but rather **scaling efficiency**: the quantum optimizer maintains tractable performance as the ensemble expands, where a classical approach would face exponential growth in subset-selection complexity.\n",
    "\n",
    "In practice:\n",
    "- Use the **classical baseline** for quick validation and benchmarking on small datasets.\n",
    "- Apply **quantum ensembles** when model width or feature complexity grows—QAOA-based search scales more gracefully in those regimes.\n",
    "- Employ **adaptive α-regularization** to maintain sparsity and generalization without increasing circuit width.\n",
    "- Monitor QPU time and depth to balance quality gains against near-term hardware constraints.\n",
    "\n",
    "Together, these experiments show that quantum-optimized ensembles complement classical methods: they reproduce baseline accuracy at small scales while offering a path to efficient scaling on larger, combinatorial learning problems. As hardware improves, these scaling advantages are expected to compound, extending the feasible size and depth of ensemble-based models beyond what is classically practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1d610-f2e2-482c-af05-a882bb380528",
   "metadata": {},
   "source": [
    "### Evaluate metrics for each configuration\n",
    "\n",
    "We now evaluate all configurations - the classical AdaBoost baseline and the three quantum ensembles - using the `evaluate_predictions` helper to compute accuracy, precision, recall, and F1 on the same test set. This comparison clarifies how quantum optimization scales relative to the classical approach: at small widths, both perform similarly; as ensembles grow, the quantum method can explore larger hypothesis spaces more efficiently. The resulting table captures these trends in a consistent, quantitative form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee8cc821-8311-4c84-8f94-ffb2ec3facc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7893333333333333\n",
      "Precision: 1.0\n",
      "Recall: 0.7893333333333333\n",
      "F1: 0.8822652757078987\n",
      "Accuracy: 0.868\n",
      "Precision: 1.0\n",
      "Recall: 0.868\n",
      "F1: 0.9293361884368309\n",
      "Accuracy: 0.8946666666666667\n",
      "Precision: 1.0\n",
      "Recall: 0.8946666666666667\n",
      "F1: 0.944405348346235\n",
      "Accuracy: 0.908\n",
      "Precision: 1.0\n",
      "Recall: 0.908\n",
      "F1: 0.9517819706498952\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Config</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost (Classical)</td>\n",
       "      <td>0.789333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.789333</td>\n",
       "      <td>0.882265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QEEC L=10, reg=7</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.929336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QEEC L=30, reg=7</td>\n",
       "      <td>0.894667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.894667</td>\n",
       "      <td>0.944405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QEEC L=60, reg=auto (α=0.82)</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.951782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Config  Accuracy  Precision    Recall        F1\n",
       "0          AdaBoost (Classical)  0.789333        1.0  0.789333  0.882265\n",
       "1              QEEC L=10, reg=7  0.868000        1.0  0.868000  0.929336\n",
       "2              QEEC L=30, reg=7  0.894667        1.0  0.894667  0.944405\n",
       "3  QEEC L=60, reg=auto (α=0.82)  0.908000        1.0  0.908000  0.951782"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Classical baseline\n",
    "acc_b, prec_b, rec_b, f1_b = evaluate_predictions(baseline_pred, y_test)\n",
    "results.append(\n",
    "    {\n",
    "        \"Config\": \"AdaBoost (Classical)\",\n",
    "        \"Accuracy\": acc_b,\n",
    "        \"Precision\": prec_b,\n",
    "        \"Recall\": rec_b,\n",
    "        \"F1\": f1_b,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Quantum runs\n",
    "for label, preds in [\n",
    "    (\"QEEC L=10, reg=7\", qeec_pred_job_1),\n",
    "    (\"QEEC L=30, reg=7\", qeec_pred_job_2),\n",
    "    (f\"QEEC L=60, reg=auto (α={REGULARIZATION_RATIO})\", qeec_pred_job_3),\n",
    "]:\n",
    "    acc, prec, rec, f1 = evaluate_predictions(preds, y_test)\n",
    "    results.append(\n",
    "        {\n",
    "            \"Config\": label,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": prec,\n",
    "            \"Recall\": rec,\n",
    "            \"F1\": f1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cecf6-2c87-4356-b9be-26f4346c194f",
   "metadata": {},
   "source": "### Augmentation du nombre d'apprenants\nIci, nous augmentons `num_learners` de 10 à 30 tout en conservant `regularization = 7`.\n\n- Plus d'apprenants élargissent l'espace d'hypothèses, permettant au modèle de capturer des motifs plus subtils, ce qui peut modestement améliorer le F1.\n- Dans la plupart des cas, la différence de temps d'exécution entre 10 et 30 apprenants n'est pas substantielle, indiquant que la largeur de circuit supplémentaire n'augmente pas significativement le coût d'exécution.\n- L'amélioration de la qualité suit toujours une *courbe de rendements décroissants* : les gains initiaux apparaissent à mesure que l'ensemble grandit, mais ils plafonnent car les apprenants supplémentaires apportent moins d'informations nouvelles.\n\nCette expérience met en évidence le compromis qualité-efficacité — augmenter la largeur de l'ensemble peut offrir de petits gains de précision sans pénalité majeure en temps d'exécution, selon le backend et les conditions de transpilation."
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f15c5fb-2450-4671-9bc2-471043414df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image src=\"../docs/images/tutorials/sml-classification/extracted-outputs/0f15c5fb-2450-4671-9bc2-471043414df2-0.avif\" alt=\"Output of the previous code cell\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(df_results))\n",
    "width = 0.35\n",
    "plt.figure(figsize=(7.6, 4.6))\n",
    "plt.bar(x - width / 2, df_results[\"Accuracy\"], width=width, label=\"Accuracy\")\n",
    "plt.bar(x + width / 2, df_results[\"F1\"], width=width, label=\"F1\")\n",
    "plt.xticks(x, df_results[\"Config\"], rotation=10)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Classical vs Quantum ensemble performance\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745cd406-5166-4eb9-8506-0eedd71e9b79",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The plot confirms the expected scaling pattern. The classical AdaBoost performs strongly for smaller ensembles but becomes increasingly costly to scale as the number of weak learners grows, because its subset-selection problem expands combinatorially. The quantum-enhanced models replicate classical accuracy at low widths and begin to surpass it as ensemble size increases, especially under adaptive α-regularization. This reflects the quantum optimizer’s ability to sample and evaluate many candidate subsets in parallel through superposition, maintaining tractable search even at higher widths. While current hardware overhead offsets some of the theoretical gains, the trend illustrates the scaling efficiency advantage of the quantum formulation. In practical terms, the classical method remains preferable for lightweight benchmarks, while quantum-enhanced ensembles become advantageous as model dimensionality and ensemble size expand, offering better trade-offs between accuracy, generalization, and computational growth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27afb68-f959-4f4a-910c-0aadafb7e18e",
   "metadata": {},
   "source": [
    "## Appendix: Scaling benefits and enhancements\n",
    "\n",
    "The scalability advantage of the `QuantumEnhancedEnsembleClassifier` arises from how the ensemble-selection process maps to quantum optimization.\n",
    "Classical ensemble learning methods, such as AdaBoost or random forests, become computationally expensive as the number of weak learners increases because selecting the optimal subset is a combinatorial problem that scales exponentially.\n",
    "\n",
    "In contrast, the quantum formulation — implemented here via the Quantum Approximate Optimization Algorithm (QAOA) — can explore these exponentially large search spaces more efficiently by evaluating multiple configurations in superposition.\n",
    "As a result, the training time does not grow significantly with the number of learners, allowing the model to remain efficient even as ensemble width increases.\n",
    "\n",
    "While current hardware introduces some noise and depth limitations, this workflow demonstrates a near-term hybrid approach where classical and quantum components cooperate: the quantum optimizer provides a better initialization landscape for the classical loop, improving convergence and final model quality.\n",
    "As quantum processors evolve, these scalability benefits are expected to extend to larger datasets, broader ensembles, and deeper circuit depths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee41a301",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [Introduction to Qiskit Functions](/docs/guides/functions)\n",
    "2. [Multiverse Computing Singularity Machine Learning](/docs/guides/multiverse-computing-singularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5785c",
   "metadata": {},
   "source": [
    "## Tutorial survey\n",
    "\n",
    "Please take a minute to provide feedback on this tutorial. Your insights will help us improve our content offerings and user experience.\n",
    "\n",
    "[Link to survey](https://your.feedback.ibm.com/jfe/form/SV_3BLFkNVEuh0QBWm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3"
  },
  "colab": {
   "cell_execution_strategy": "setup"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}